# ğŸ¤– No-Code ML Platform

Eine vollstÃ¤ndige Machine Learning Platform, die es ermÃ¶glicht ohne Programmierkenntnisse ML-Modelle zu trainieren und bereitzustellen.

## âœ¨ Features

### ğŸ”§ **Flexible Algorithmus-Auswahl**
- **Klassifikation**: Random Forest, Logistic Regression, SVM, XGBoost
- **Regression**: Random Forest, Linear Regression, SVR, XGBoost
- Automatische Algorithmus-Empfehlungen basierend auf Datentypen
- KomplexitÃ¤ts-Bewertung fÃ¼r jeden Algorithmus

### ğŸ“Š **Intelligente Datenverarbeitung**
- **CSV-Upload & Analyse**: Automatische Datentyp-Erkennung
- **Erweiterte Preprocessing-Pipeline**: Skalierung, One-Hot-Encoding, Label-Encoding
- **Smart Data Cleaning**: Behandlung fehlender Werte
- **Feature-Engineering**: Automatische Preprocessing je nach Datentyp

### ğŸ¯ **Erweiterte Metriken**
- **Klassifikation**: Accuracy, Precision, Recall, F1-Score, AUC-ROC, Classification Report
- **Regression**: MAE, MSE, RMSE, RÂ², MAPE
- **Feature Importance**: Visualisierung der wichtigsten Features

### ğŸ§  **Dynamische Multi-Agent-Orchestrierung**
- **Autonome Agent-Entscheidungen**: Agents entscheiden eigenstÃ¤ndig Ã¼ber nÃ¤chste Schritte
- **Event-basiertes Tracking**: Real-time Verfolgung aller Agent-AktivitÃ¤ten
- **Intelligente Ãœbergaben**: Agents kÃ¶nnen an jeden anderen Agent Ã¼bergeben
- **Adaptive Pipeline**: Keine starre Reihenfolge - dynamische Workflows
- **Zentrale Agent-Konfiguration**: Jeder Agent kann eigenes LLM-Modell verwenden
- **Zero-Template-Ansatz**: VollstÃ¤ndig generierte Scripts statt starrer Templates
- **Lokale LLM-Provider**: VollstÃ¤ndig lokale Implementierung mit Ollama

### ğŸš€ **REST-API**
- **Training-API**: Automatisches Model-Training mit echten Python-Scripts (Worker-Pool)
- **Prediction-API**: Vorhersagen mit trainierten Modellen zur externen Nutzung und zu Testzwecken
- **Model-Export**: Download der trainierten .pkl-Modelle
- **Persistente Speicherung**: SQLite-Datenbank fÃ¼r alle Projekte
- **Monitoring & Scaling**: Endpoints fÃ¼r Queue-, Worker- und Scaling-Status
- **File & Cache Management**: Endpoints fÃ¼r Dateien, Analyse-Cache und Predict-Cache

### ğŸ¨ **Moderne BenutzeroberflÃ¤che**
- Intuitive 3-Schritte Wizard fÃ¼r Projekt-Erstellung
- Echzeit-Datei-Analyse mit Spalten- und Zeilenzahl
- Live-Training-Status mit Polling
- Performance-Visualisierung mit Charts
- Responsive Design

## ğŸ—ï¸ Architektur

```
ML-Platform/
â”œâ”€â”€ backend/                      # Node.js + Express + SQLite
â”‚   â”œâ”€â”€ server.js                 # Haupt-Server mit REST-API
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ api/                  # Endpoints (projects, upload, analyze, predict, files, agents, ...)
â”‚   â”‚   â”œâ”€â”€ execution/            # Python-Exec, Code-Gen, Worker, Predict-Cache
â”‚   â”‚   â”œâ”€â”€ llm/                  # Dynamische Agents, Agent-Config, Queue, Tuning
â”‚   â”‚   â”œâ”€â”€ monitoring/           # Job-Queue, Scaling-Monitor, Logs
â”‚   â”‚   â””â”€â”€ config/               # Worker-Scaling-Konfiguration
â”‚   â”œâ”€â”€ models/                   # Gespeicherte .pkl-Modelle
â”‚   â”œâ”€â”€ scripts/                  # Generierte Python-Scripte (Train/Predict)
â”‚   â”œâ”€â”€ uploads/                  # Hochgeladene CSV-Dateien
â”‚   â””â”€â”€ services/python/venv/     # Virtuelle Umgebung fÃ¼r Python
â”œâ”€â”€ frontend/                     # React + TypeScript
â”‚   â”œâ”€â”€ components/               # UI-Komponenten
â”‚   â”œâ”€â”€ services/                 # API-Services
â”‚   â””â”€â”€ types.ts                  # TypeScript-Definitionen
â””â”€â”€ README.md
```

## ğŸš€ Installation & Start

### Voraussetzungen
- Node.js (Version 16+)
- Python 3.x

Richte vor dem Start ein virtuelles Python-Environment unter `backend/services/python/venv` ein und installiere die Requirements aus `backend/requirements.txt`:

```bash
# Windows (PowerShell)
python -m venv backend/services/python/venv
backend/services/python/venv/Scripts/pip install -r backend/requirements.txt

# macOS/Linux
python3 -m venv backend/services/python/venv
backend/services/python/venv/bin/pip install -r backend/requirements.txt
```

### Backend starten
```bash
cd backend
npm install
npm run dev          # Entwicklungsserver auf Port 3001
```

### Frontend starten  
```bash
cd frontend
npm install
npm run dev          # Entwicklungsserver auf Port 5173
```

## ğŸ“¡ API Endpoints

### ğŸ”„ Projekt-Management
```http
GET    /api/projects                          # Alle Projekte
GET    /api/projects/:id                      # Projekt-Details
POST   /api/projects                          # Projekt erstellen (klassisch)
POST   /api/projects/smart-create             # Projekt mit LLM-Empfehlungen
PUT    /api/projects/:id/code                 # Python-Code (+ optional Hyperparameter) aktualisieren
POST   /api/projects/:id/retrain              # Re-Training mit aktuellem Code
POST   /api/projects/:id/evaluate-performance # LLM-gestÃ¼tzte Performance-Insights
GET    /api/projects/:id/data-statistics      # Erweiterte Datenstatistiken zum Projekt
GET    /api/projects/:id/stats                # Basis-Stats zur Quelldatei
GET    /api/projects/:id/download             # Modell (.pkl) herunterladen
DELETE /api/projects/:id                      # Projekt lÃ¶schen
```

### ğŸ“¤ Datei-Upload & Analyse
```http
POST   /api/upload                 # Datei hochladen & Basisanalyse
POST   /api/analyze-data           # LLM-Empfehlungen auf manipulierte Spalten anwenden
POST   /api/explore-data           # Automatische Datenexploration (Cache-gestÃ¼tzt)
```

### ğŸ¯ Prediction
```http
POST   /api/predict/:id            # Vorhersage mit trainiertem Modell
```

### ğŸ¤– LLM-Management
```http
GET    /api/llm/config             # Aktuelle LLM-Konfiguration
GET    /api/llm/status             # Ollama-Status
GET    /api/llm/ollama/models      # VerfÃ¼gbare Ollama-Modelle
POST   /api/llm/ollama/test        # Ollama-Verbindung testen
POST   /api/llm/ollama/config      # Ollama-Host/Default-Model anpassen
```

### ğŸ¤– Agent-Management
```http
GET    /api/agents                 # Alle verfÃ¼gbaren Agents abrufen
GET    /api/agents/:agentKey       # Spezifische Agent-Konfiguration
PUT    /api/agents/:agentKey/model # Agent-Modell aktualisieren
PUT    /api/agents/models/bulk     # Mehrere Agent-Modelle auf einmal Ã¤ndern
PUT    /api/agents/models/all      # Alle Agents auf dasselbe Modell setzen
GET    /api/agents/stats           # Agent-Statistiken
POST   /api/agents/:agentKey/test  # Agent-Konfiguration testen

GET    /api/agents/activities      # Alle Agent-Activities
GET    /api/agents/activities/:projectId  # Agent-Activities fÃ¼r Projekt
GET    /api/agents/active          # Aktuell aktive Agents
DELETE /api/agents/activities/:projectId # Agent-Activities lÃ¶schen
GET    /api/agents/activities/:projectId/stream # Real-time Agent-Updates
```

### ğŸ“ˆ Monitoring
```http
POST   /api/projects/:id/monitoring/init      # Baseline initialisieren
POST   /api/projects/:id/monitoring/event     # Prediction-Event loggen (optional mit truth)
GET    /api/projects/:id/monitoring/status    # Monitoring-Status abrufen
POST   /api/projects/:id/monitoring/reset     # Monitoring zurÃ¼cksetzen
```

### ğŸ—‚ï¸ Datei-Management
```http
GET    /api/files/:type                 # Dateien auflisten (scripts|models|uploads)
GET    /api/files/:type/:filename       # Datei-Info abrufen
DELETE /api/files/:type                 # Datei lÃ¶schen (Body: { filePath })
GET    /api/files/storage/stats         # Aggregierte Speicher-Statistiken
```

### ğŸ§  Analyse-/File-Cache
```http
POST   /api/cache/clear                 # (Legacy) File-Cache Nachricht
GET    /api/cache/status                # (Legacy) File-Cache Status

POST   /api/analysis-cache/clear        # Datenanalyse-Cache leeren
GET    /api/analysis-cache/status       # Datenanalyse-Cache Status
```

### âš¡ Predict-Cache
```http
POST   /api/predict-cache/cleanup       # Alte Predict-Skripte bereinigen
GET    /api/predict-cache/status        # Ãœberblick Ã¼ber gecachte Predict-Skripte
DELETE /api/predict-cache/project/:projectId   # Cache fÃ¼r Projekt lÃ¶schen
DELETE /api/predict-cache/all           # Gesamten Predict-Cache leeren
```

### ğŸ“Š Scaling & Queue/Worker
```http
GET    /api/scaling/metrics             # Live-Skalierungsmetriken
GET    /api/scaling/report              # Detaillierter Report
GET    /api/scaling/history/:type       # Verlauf (type: python|llm)
GET    /api/scaling/utilization/:type   # Auslastungsanalyse
POST   /api/scaling/config              # Skalierungs-Konfiguration Ã¤ndern
GET    /api/scaling/status              # Zusammengefasster Status fÃ¼r Dashboard

GET    /api/llm/queue/status            # LLM-Queue Status
POST   /api/llm/queue/cancel/:requestId # LLM-Request abbrechen

GET    /api/worker/queue-status         # Job-Queue + Workerpool Status
GET    /api/worker/jobs                 # Jobs (limit optional)
GET    /api/worker/jobs/:type           # Jobs nach Typ
GET    /api/worker/job/:jobId           # Einzelner Job
POST   /api/worker/job/:jobId/cancel    # Job abbrechen
GET    /api/worker/stats                # Worker/Queue Kennzahlen
```

## ğŸ”¬ Verwendung

### 1ï¸âƒ£ **Projekt erstellen**
- CSV-Datei hochladen
- Automatische Datenanalyse 
- Algorithmus auswÃ¤hlen
- Zielvarible und Features festlegen

### 2ï¸âƒ£ **Training starten**
- **Dynamische Multi-Agent-Pipeline** mit autonomen Entscheidungen
- **Agent-zu-Agent-Kommunikation** fÃ¼r optimale Code-Generierung
- **Real-time Agent-Tracking** im Frontend
- **Adaptive Preprocessing-Pipeline** je nach Datentypen
- **Echte scikit-learn/XGBoost AusfÃ¼hrung** mit optimierten Parametern
- **Live-Status-Updates** wÃ¤hrend des Trainings
- **Performance-Metriken** werden automatisch extrahiert

### 3ï¸âƒ£ **Modell nutzen**
- **Predictions**: Ãœber UI oder API-Endpoint
- **Export**: .pkl-Datei fÃ¼r lokale Nutzung
- **Analysis**: Performance-Charts und Feature Importance

## ğŸ¯ API-Beispiele

### Prediction-Request
```bash
curl -X POST 'http://localhost:3001/api/predict/{PROJECT_ID}' \
  -H 'Content-Type: application/json' \
  -d '{
    "features": {
      "age": 35,
      "income": 50000,
      "experience": 10
    }
  }'
```

### Response
```json
{
  "prediction": "1"
}
```

## ğŸ”§ Technologie-Stack

### Backend
- **Node.js** + **Express.js** - REST-API Server
- **SQLite** - Persistente Datenspeicherung  
- **Python Integration** - Echte ML-Pipeline mit scikit-learn
- **Worker Threads** - Python Worker Pool mit Auto-Scaling
- **Multer** - File-Upload-Handling
- **Logging** - REST & LLM Kommunikation

### Frontend  
- **React** + **TypeScript** - Moderne UI
- **Vite** - Build-Tool
- **Recharts** - Performance-Visualisierung
- **Tailwind CSS** - Styling

### Machine Learning
- **scikit-learn** - Standard ML-Algorithmen
- **XGBoost** - Gradient Boosting
- **pandas** - Datenmanipulation
- **joblib** - Model-Serialisierung

### LLM
- **Ollama** (Lokal) mit z. B. `mistral:latest`
- **LangGraph/LangChain** fÃ¼r Multi-Agent Orchestrierung (Code-Gen/Review)

## âš™ï¸ Umgebungsvariablen

- `PORT` (optional, Default: `3001`)
- `OLLAMA_URL` (Default: `http://127.0.0.1:11434`)

## ğŸ¨ Screenshots

### Performance-Dashboard  
![Dashboard](docs/performance.png)

### Demo  
![Demo](docs/Demo-LLM2ML-09092025.mp4)
